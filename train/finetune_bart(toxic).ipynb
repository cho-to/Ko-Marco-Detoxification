{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ab397368248846748b6ead83b2c6c120":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84b294acad8e48b48012260d453effe2","IPY_MODEL_1c8774a19d844b28ba028a91c38a7111","IPY_MODEL_127c055483984bc593c4c12e7c7e7fa2"],"layout":"IPY_MODEL_ef71c5a5c0ac4232a0259d6e4f3bbbaf"}},"84b294acad8e48b48012260d453effe2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f473573a000407fb1a98bf1ac452f6a","placeholder":"â€‹","style":"IPY_MODEL_1b4278870ca24d2fa59dbfa68d4b767b","value":"model.safetensors:â€‡100%"}},"1c8774a19d844b28ba028a91c38a7111":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d6022b7492c4cf7866580add4093234","max":495589768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e964cdc642924fcea1b703265cf76786","value":495589768}},"127c055483984bc593c4c12e7c7e7fa2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6687efa93a914cf79ac9d3ec245025cd","placeholder":"â€‹","style":"IPY_MODEL_2a5fcfa75acf4bc3a793381801c22a94","value":"â€‡496M/496Mâ€‡[00:53&lt;00:00,â€‡5.01MB/s]"}},"ef71c5a5c0ac4232a0259d6e4f3bbbaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f473573a000407fb1a98bf1ac452f6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b4278870ca24d2fa59dbfa68d4b767b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d6022b7492c4cf7866580add4093234":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e964cdc642924fcea1b703265cf76786":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6687efa93a914cf79ac9d3ec245025cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a5fcfa75acf4bc3a793381801c22a94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c478eb1a330430b914c885d110ac34e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f5aa818c7fb4a8eae8619d1996d08dc","IPY_MODEL_4a49568624a44e0a804a317130ce923b","IPY_MODEL_16056938be65493b84d1dfd29ae161e9"],"layout":"IPY_MODEL_ad7ca83b6b784befa6f2fbdd34e7a89a"}},"7f5aa818c7fb4a8eae8619d1996d08dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bec89f4f2f246a7968af257f95ad3cc","placeholder":"â€‹","style":"IPY_MODEL_411e0dea5a214d6a9ecbc958efcdef71","value":"README.md:â€‡100%"}},"4a49568624a44e0a804a317130ce923b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea886dee5784781b29ed24020e35bbd","max":5174,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35ddeb5173394684a2da229bfbeb2bfc","value":5174}},"16056938be65493b84d1dfd29ae161e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c60578887b04018bd59d6b283d9cd36","placeholder":"â€‹","style":"IPY_MODEL_ca5ebd8b6e744a85b0ca67937799152c","value":"â€‡5.17k/5.17kâ€‡[00:00&lt;00:00,â€‡303kB/s]"}},"ad7ca83b6b784befa6f2fbdd34e7a89a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bec89f4f2f246a7968af257f95ad3cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"411e0dea5a214d6a9ecbc958efcdef71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ea886dee5784781b29ed24020e35bbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35ddeb5173394684a2da229bfbeb2bfc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c60578887b04018bd59d6b283d9cd36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca5ebd8b6e744a85b0ca67937799152c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8_2owVxZDXM","executionInfo":{"status":"ok","timestamp":1726751277608,"user_tz":-540,"elapsed":2752,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"4ff2285b-ef73-4ecc-956a-e2b5d2eaa9ae"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install sacremoses\n","!pip install ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Kl-RdLrMbl0","executionInfo":{"status":"ok","timestamp":1726751353244,"user_tz":-540,"elapsed":7934,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"25e3178e-b345-4128-ab75-3477244cb2e4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.2.3)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n"]}]},{"cell_type":"code","source":["# Code for infilling and adding masks to sequence (used to randomly mask sequences in BART training)\n","\n","import numpy as np\n","import nltk.tokenize.casual\n","import bisect\n","from IPython import embed\n","\n","np.random.seed(0)\n","\n","def max_span(num_mask, tokenized_len, thresh):\n","    i = 1\n","    while True:\n","        if (num_mask + i)/(tokenized_len) >= thresh:\n","            return i\n","        i += 1\n","\n","def list_diffs(arr, max_len):\n","    if len(arr) == 0:\n","        return max_len\n","    else:\n","        # print(np.diff(np.array([0] + arr + [max_len - 1]) - 2))\n","        return np.max(np.diff(np.array([0] + arr + [max_len - 1]) - 2))\n","\n","def collapse_contig(arr, token):\n","    output = []\n","    seen_prev = False\n","    for i in arr:\n","        if i == token:\n","            if seen_prev:\n","                seen_prev = True\n","                continue\n","            seen_prev = True\n","        else:\n","            seen_prev = False\n","        output.append(i)\n","    return output\n","\n","def text_infill(sentence, mask_token, lam = 3, thresh = 0.3):\n","    tokenized = np.array(nltk.tokenize.casual.casual_tokenize(sentence), dtype = \"object\")\n","    masked_idcs = []\n","\n","    while (len(masked_idcs) / len(tokenized)) < thresh:\n","        span_length = np.random.poisson(lam = lam)\n","\n","        while ((span_length > list_diffs(masked_idcs, len(tokenized))) or \\\n","            (span_length > max_span(len(masked_idcs), len(tokenized), thresh))):\n","            span_length = np.random.poisson(lam = lam)\n","            # print(\"Span length is too long, it is currently:\", span_length)\n","\n","        # print(\"tokenized is\", tokenized)\n","        # print(\"masked idcs are\", masked_idcs)\n","        # print(\"span length is\", span_length)\n","\n","        if span_length == 0:\n","            start_idx = np.random.randint(0, len(tokenized) + 1)\n","            while ((start_idx in masked_idcs) or (start_idx in (np.array(masked_idcs) + 1))):\n","                # print(\"bad, start_idx is\", start_idx)\n","                start_idx = np.random.randint(0, len(tokenized) + 1)\n","\n","            # print(\"start idx is\", start_idx)\n","            tokenized = np.insert(tokenized, start_idx, mask_token)\n","            bisect.insort(masked_idcs, start_idx)\n","\n","        else:\n","            while True:\n","                start_idx = np.random.randint(0, len(tokenized) - span_length + 1)\n","                idcs = np.arange(start_idx, start_idx + span_length)\n","\n","                for i in idcs:\n","                    if i in masked_idcs or i in (np.array(masked_idcs) + 1):\n","                        # print(\"bad i\" , i)\n","                        continue\n","                break\n","\n","            for i in idcs:\n","                bisect.insort(masked_idcs, i)\n","                tokenized[i] = mask_token\n","            #print(\"idcs are\", idcs)\n","    # print(\"final mask ratio:\",len(masked_idcs)/len(tokenized))\n","    return collapse_contig(tokenized, mask_token)\n","\n","# Masks tokens from idx to idx + span_length with mask_token\n","# If idx > length of sequence, does not mask anything\n","def span_mask(sentence, idx, mask_token, span_length = 1):\n","    tokenized = np.array(nltk.tokenize.casual.casual_tokenize(sentence), dtype = \"object\")\n","    max_len = len(tokenized)\n","    if idx < max_len:\n","        end_span = min(idx+span_length, max_len)\n","        tokenized[idx:end_span] = [mask_token for i in range(end_span - idx)]\n","    return tokenized\n","\n","# embed()\n","# print(text_infill(\"I'm gonna go, do you want anything Mom?\", \"<mask>\"))\n","# print(text_infill(\"Hey I'm going to the store, do you want anything?\", \"<mask>\"))"],"metadata":{"id":"glRKk3VdMruZ","executionInfo":{"status":"ok","timestamp":1726751359394,"user_tz":-540,"elapsed":2607,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n","from sacremoses import MosesDetokenizer\n","import numpy as np\n","import torch\n","import random\n","import html\n","import re\n","import ftfy\n","from nltk.tokenize.casual import casual_tokenize\n","\n","nl_tok = \"[<NEW>]\"\n","md = MosesDetokenizer(lang='ko')\n","\n","def detokenize(input):\n","    # return TreebankWordDetokenizer().detokenize(input)\n","    return md.detokenize(input)\n","\n","def set_seed(seed, n_gpu):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(seed)\n","\n","def bool2str(cand):\n","    if cand:\n","        return \"T\"\n","    return \"F\"\n","\n","def seed_everything(seed = 0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # Only useful for convolution\n","    # torch.backends.cudnn.deterministic = True\n","    # torch.backends.cudnn.benchmark = False\n","\n","def preprocess(text, preserve_lines = False):\n","    if preserve_lines:\n","        return ftfy.fix_text(html.unescape(text))\n","    # Remove linee break and excess spaces\n","    return ftfy.fix_text(html.unescape(re.sub(r'\\s+', ' ', text).strip()))\n","\n","# Quick test\n","# TreebankWordDetokenizer.detokenize(TreebankWordTokenizer.tokenize(\"sh*t\"))"],"metadata":{"id":"LK8nnTK_XX8d","executionInfo":{"status":"ok","timestamp":1726751367122,"user_tz":-540,"elapsed":3438,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"8WbGUUFLF9_e","executionInfo":{"status":"ok","timestamp":1726751376352,"user_tz":-540,"elapsed":6258,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"outputs":[],"source":["# Finetuning the toxic and nontoxic language models\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, AdamW, EarlyStoppingCallback, PreTrainedTokenizerFast\n","import torch\n","import numpy as np\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import argparse\n","import random\n","from IPython import embed"]},{"cell_type":"code","source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","if not torch.cuda.is_available():\n","    print(\"No GPUs found!\")\n","else:\n","    print(\"Found\", str(torch.cuda.device_count()), \"GPUS!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aa46ild7MC3Y","executionInfo":{"status":"ok","timestamp":1726751378779,"user_tz":-540,"elapsed":645,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"adf82609-5953-487e-b4e4-49e4e2b910c9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1 GPUS!\n"]}]},{"cell_type":"code","source":["# Load in the tokenizer\n","tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n","model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1', forced_bos_token_id = tokenizer.bos_token_id).to(device)\n","model.train()\n","mask = tokenizer.mask_token\n","\n","model_dir = '/content/drive/MyDrive/á„Œá…©á†¯á„Œá…¡á†¨/model/toxic'\n","\n","if not os.path.exists(model_dir):\n","    print(model_dir)\n","    os.mkdir(model_dir)\n","\n","output_dir = model_dir\n","print(output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YWWBnz4OWmQ","executionInfo":{"status":"ok","timestamp":1726751415111,"user_tz":-540,"elapsed":2945,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"e24be4e2-1ec8-42f1-9687-8ca3a85f3f8e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/á„Œá…©á†¯á„Œá…¡á†¨/model/toxic\n"]}]},{"cell_type":"code","source":["train_texts = []\n","val_texts = []\n","\n","# Read/process the data based on which dataset we're using: Jigsaw or Dynabench\n","# If you want to load your own data, put the data loading logic here\n","train = pd.read_csv('/content/unsmile_train_v1.0.tsv',delimiter='\\t')\n","val = pd.read_csv('/content/unsmile_valid_v1.0.tsv',delimiter='\\t')\n","\n","train_texts =  train[\"ë¬¸ì¥\"].tolist()\n","val_texts = val[\"ë¬¸ì¥\"].tolist()\n","\n","print(len(train_texts), len(val_texts))\n","print(train_texts[0], val_texts[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLsiTEHfRU2j","executionInfo":{"status":"ok","timestamp":1726751418478,"user_tz":-540,"elapsed":501,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"4cc5f717-3dc6-46ad-825d-c22c51bf2f15"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["15005 3737\n","ì¼ì•ˆí•˜ëŠ” ì‹œê°„ì€ ì‰¬ê³ ì‹¶ì–´ì„œ ê·¸ëŸ°ê²Œ ì•„ë‹ê¹Œ ã…‡ã„±ã„¹ã…‡ ì§„ì§œ ì£½ì–´ë„ ìƒê´€ì—†ë‹¤ëŠ” ë§ˆì¸ë“œë¡œ ì‹¸ì› ë”ë‹ˆ ì§€ê¸ˆ ì„œì—´ ìƒíƒ€ì·¨ë…¸ ì‹ì¹¼ë“¤ê³  ë‹ˆê°€ ë‚˜ ì•ˆì°Œë¥´ë©´ ë‚´ê°€ ë„ˆ ì°Œë¥¸ë‹¤ í–ˆë”ë‹ˆ ì• ë¹„ì¶© ëƒ„ë™ì¶© ì•Œì•„ì„œê¸°ë…¸ ^ë«^ í•œë²ˆ ì„œì—´ ì¡ê³  ê·¸ ë‹´ì— ê°œê¸¸ë•Œë§ˆë‹¤ ë” ì„¸ê²Œë‚˜ê°€ë©´ í™•ì‹¤í•˜ê²Œ ì§“ëˆ„ë¥¼ìˆ˜ìˆë‹¤ìµì´\n"]}]},{"cell_type":"code","source":["# # Test percentiles of tokenized lengths\n","# src_lengths = [len(tokenizer(x).input_ids) for x in train_texts]\n","# # tgt_lengths = [len(tokenizer(x).input_ids) for x in train_labels]\n","# print(np.percentile(src_lengths, 99))\n","# embed()\n","\n","# Tokenize everything\n","tokenized_labs_train = tokenizer.batch_encode_plus(\n","    train_texts,\n","    max_length = 232, # args.max_target_length\n","    padding=\"max_length\",\n","    truncation=True,\n","    return_tensors = \"pt\").input_ids\n","\n","tokenized_labs_val = tokenizer.batch_encode_plus(\n","    val_texts,\n","    max_length = 232, # args.max_target_length,\n","    padding=\"max_length\",\n","    truncation=True,\n","    return_tensors = \"pt\").input_ids\n","\n","tokenized_labs_val[tokenized_labs_val == tokenizer.pad_token_id] = tokenizer.eos_token_id\n","tokenized_labs_train[tokenized_labs_train == tokenizer.pad_token_id] = tokenizer.eos_token_id"],"metadata":{"id":"1KnpXHx6TOZ0","executionInfo":{"status":"ok","timestamp":1726751429711,"user_tz":-540,"elapsed":6417,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[" # Convert this so that examples are dynamically masked\n","class JigsawDataset(torch.utils.data.Dataset):\n","    def __init__(self, labels, rawdata):\n","        self.rawdata = rawdata\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        # Dynamically generate sources\n","        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        # item['labels'] = cur_lab\n","\n","        # Do detokenization to ensure that the tokenization matches up later\n","        item = tokenizer.encode_plus(\n","            detokenize(text_infill(self.rawdata[idx], tokenizer.mask_token)),\n","            max_length = 182, # args.max_source_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors = \"pt\")\n","\n","        # Necessary for squeezing purposes\n","        item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","        item['labels'] = self.labels[idx]\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = JigsawDataset(tokenized_labs_train, train_texts)\n","val_dataset = JigsawDataset(tokenized_labs_val, val_texts)"],"metadata":{"id":"jpS56IZcVkHU","executionInfo":{"status":"ok","timestamp":1726751433083,"user_tz":-540,"elapsed":503,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Print out what the model looks like before we start training\n","print(\"Before training:\")\n","inputs_masked = [\"í•œêµ­ì¸ë“¤ì€ <mask>\"]\n","print(\"\\n Generations: \", tokenizer.batch_decode(model.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to('cuda').input_ids, do_sample=True, num_return_sequences = 3)),\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwltwwONXuRs","executionInfo":{"status":"ok","timestamp":1726751512910,"user_tz":-540,"elapsed":637,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"179ebb7f-04b9-4992-ac0e-d94572664c71"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Before training:\n","\n"," Generations:  ['</s><s>          ê²Œ       </s>', '</s><s> ì´ë‚       ëŒ€ë¶€ë¶„   ì´       </s>', '</s><s>               ì¶œêµ­  </s>'] \n","\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(train_dataset[100]['input_ids']))\n","print(tokenizer.decode(train_dataset[100]['labels']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXXBNX1mMfdt","executionInfo":{"status":"ok","timestamp":1726751582309,"user_tz":-540,"elapsed":483,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"bc9c7292-4cb0-43fa-ecc1-27db1fb5375e"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["ì´ê±° ìƒê°í•´ë³´ë‹ˆ ì§„ì§œ íˆ¬ì‹ ìì‚´ì¼ê¹Œ <mask> ë°€ì–´ì„œ ë–¨ì–´íŠ¸ë¦°ê±° ì•„ë‹ˆë…¸...?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","ì´ê±° ìƒê°í•´ë³´ë‹ˆ ì§„ì§œ íˆ¬ì‹ ìì‚´ì¼ê¹Œ í•˜ëŠ” ìƒê°ì´ ë“œë…¸.. ê°•ê°„í•˜ë ¤ë‹¤ê°€ ë°€ì–´ì„œ ë–¨ì–´íŠ¸ë¦°ê±° ì•„ë‹ˆë…¸...?</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n"]}]},{"cell_type":"code","source":["class BartTrainer(Trainer):\n","        def compute_loss(self, model, inputs, return_outputs=False):\n","            outputs = model(\n","                input_ids = inputs.get(\"input_ids\"),\n","                attention_mask = inputs.get(\"attention_mask\"),\n","                labels = inputs.get(\"labels\")\n","                )\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            return (loss, outputs) if return_outputs else loss\n","\n","        def evaluate(self, **kwargs):\n","            metrics = super().evaluate(**kwargs)\n","            # Example to see what the model outputs for mask token (see if it outputs something biased)\n","            inputs_masked = [\"í•œêµ­ì¸ë“¤ì€ <mask>\"]\n","            print(\"\\n Generations: \", tokenizer.batch_decode(model.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to('cuda').input_ids, do_sample=True, num_return_sequences = 5)),\"\\n\")\n","            return metrics"],"metadata":{"id":"U1R3nEnoYf6K","executionInfo":{"status":"ok","timestamp":1726751590927,"user_tz":-540,"elapsed":641,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=output_dir,          # output directory\n","    max_steps= 30000, # args.max_steps,              # total number of training steps\n","    per_device_train_batch_size=8, # args.train_batch_size,  # batch size per device during training\n","    per_device_eval_batch_size=64, # args.eval_batch_size,   # batch size for evaluation\n","    learning_rate =2.5e-5, # args.lr,\n","    evaluation_strategy = \"steps\",\n","    save_strategy = \"steps\",\n","    save_steps = 1000, # args.save_steps,\n","    eval_steps = 500, # args.save_steps,\n","    fp16 = True, # args.fp16,\n","    metric_for_best_model = \"eval_loss\",\n","    greater_is_better = False,\n","    load_best_model_at_end = True,\n","    save_total_limit = 2, # args.save_total_limit,\n","    logging_dir='/content/logs', # args.logging_dir,            # directory for storing logs\n","    logging_steps= 500, #args.logging_steps,\n","    seed = 0, # args.seed,\n","    save_safetensors=False\n",")\n","\n","trainer = BartTrainer(\n","    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset,             # evaluation dataset\n","    callbacks = [EarlyStoppingCallback(5)] # args.early_stopping_steps\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vMdcc6hcYjB2","outputId":"b2c07e71-8e19-4dd9-870f-55bd06a19ed5","executionInfo":{"status":"ok","timestamp":1726753639158,"user_tz":-540,"elapsed":2045102,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}}},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","max_steps is given, it will override any value given in num_train_epochs\n","<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='9500' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9500/30000 34:02 < 1:13:29, 4.65 it/s, Epoch 5/16]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.254800</td>\n","      <td>0.172361</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.177100</td>\n","      <td>0.166343</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.173100</td>\n","      <td>0.162991</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.168200</td>\n","      <td>0.162381</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.158000</td>\n","      <td>0.158967</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.157100</td>\n","      <td>0.158048</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.160500</td>\n","      <td>0.156970</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.155100</td>\n","      <td>0.161789</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.151500</td>\n","      <td>0.155781</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.151200</td>\n","      <td>0.156322</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.149600</td>\n","      <td>0.154160</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.142900</td>\n","      <td>0.154833</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.142400</td>\n","      <td>0.153920</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.143200</td>\n","      <td>0.153039</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.143900</td>\n","      <td>0.154678</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.136700</td>\n","      <td>0.153325</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.139100</td>\n","      <td>0.154649</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.138300</td>\n","      <td>0.155522</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.134700</td>\n","      <td>0.153188</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì €ê²Œë­ì•¼?</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ë“¤ì€ ì• êµì‹¬ ë§ìŒ</s><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ë“¤ì€ ì˜ ì•ˆë˜ë„¤!</s><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ë“¤ì€ ì™œ?</s><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ê·¸ë†ˆë“¤ì€ ì•„í”Œê±°ì•„ë‹Œê°€</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì• ë“¤ë„ ë‹¤ë‹Œë‹¤</s><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸í•œí…Œ ì–´ë–»ê²Œ í•˜ë©´í•˜ì§€</s><pad>', '</s><s>ì¸ë“¤ì€ ì§€ë“¤ë¼ë¦¬...</s><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ë“¤ì€ ì €ê²Œ ë­ëƒ?</s>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ë“¤ì€ ì¢†ë‚˜ë‹¤.</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ í•œêµ­ì‚¬ëŒì´ë‚˜ ë˜ëŠ”ì¤„</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸</s><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì¼ë³¸ì´ë‚˜ ê°”ë‹¤ì™€</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì—¬ìì„</s><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì „ë¼ë„ê³  ì§€ë“¤ á„á„á„</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì§±ê¹¨ìƒˆë¼ë“¤ì„</s>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ì„ í˜ì˜¤í•œë‹¤</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ê°œìŠ¬ëŒìƒˆë¼ë“¤ì´</s><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ë§Œ á„á„á„</s><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ì´ë‹¤...</s><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì¢†ê°™ì€ ê°œë…</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ë³‘ì‹ ê°™ë…¸ ì§±ê°œë“¤</s>', '</s><s>ì¸ë“¤ì€ ë¬´ì§€ë§‰ì§€í–ˆë„¤</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ìëŒ•ì´ ìƒˆë¼</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì‹œë°œ á„á„</s><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì–´ì´ì—†ë„¤</s><pad>', '</s><s>ì¸ë“¤ì€ ì¬ê¸°ë‚˜ì˜¬ê¹Œ?</s>', '</s><s>ì¸ë“¤ì€ ì™œ í•œêµ­ê°€ëƒ</s><pad>', '</s><s>ì¸ë“¤ì€ í˜ë¯¸ì¸ë°</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ì¼ë² ê°€ ì•„ë‹ˆë‹¤</s><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ê°œìŠ¬ëŒì´ë‹¤.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ á„á„á„</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ á„á„á„á„á„á„á„á„á„á„á„á„á„á„á„</s>', '</s><s>ì¸ë“¤ì€ á„á„á„á„</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì™œ ë’ˆì¡Œë…¸á„á„á„á„á„á„á„á„á„á„á„</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ìì§€ê°€ ë™ì„±ì• ìì¼ë¿</s>', '</s><s>ì¸ë“¤ì€ ê± ë¯¿ì–´ì•¼í•œë‹¤</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ë”ëŸ½ë„¤</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì§€ê°€ ì¢‹ì•„í•œë‹¤</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ëª»ìƒê²¼ì–´</s><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì™œì´ëŸ¬ì§€ á„á„</s>', '</s><s>ì¸ë“¤ì€ ì–´ë”¨ëƒ?</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ë³´ë‹¤ ë‚«ë…¸</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ á„á„á„á„</s><pad><pad>', '</s><s>ì¸ë“¤ì€ á„…á„‹</s><pad><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ á„á„á„á„</s>', '</s><s>ì¸ë“¤ì€ í•œë‚¨ë‹µì–´ë¼</s>', '</s><s>ì¸ë“¤ì€ ì§€ì˜¥</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì „ë¼ë„ì•¼</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ì¢†ë‚˜ë³´ë„¤</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ á„á„á„á„á„á„á„á„á„á„á„á„á„á„á„</s>', '</s><s>ì¸ë“¤ì€ á„á„á„á„á„á„á„á„á„á„á„á„á„á„á„</s>', '</s><s>ì¸ë“¤ì€ ìŸ¤ë„¤ë“¤ì´ì•„</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ á„á„á„á„á„á„á„á„á„á„á„á„á„á„á„</s>', '</s><s>ì¸ë“¤ì€ ì¢†ë¬¼ê°™ì•„ìš”</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ë§í˜¼í•©í•©ë‹ˆë‹¤</s>', '</s><s>ì¸ë“¤ì€ ì „ë¼ë„ì•„ë‹ˆë¼</s><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ë•…</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì£½ê² ë…¸</s><pad><pad>', '</s><s>ì¸ë“¤ì€ á„á„</s><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì§±ê¹¨ê°™ì€ìƒˆí‚¤</s>', '</s><s>ì¸ë“¤ì€ ì  ì‹ ë³‘ì</s><pad>', '</s><s>ì¸ë“¤ì€ ê°œë…ì¸ê°€ë³´ë„¤</s><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸</s><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ë³‘ì‹ ì´ë…¸</s><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ë¯¸ì³¤ë‹¤</s><pad>', '</s><s>ì¸ë“¤ì€ ì¡°ì„ ì¡±</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ë­ëƒá„€á„€</s>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì€ ì‚¬ë‚˜ì´ì„</s>', '</s><s>ì¸ë“¤ì€ êµíšŒë¥¼ê°€ë…¸</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ á„Œá„‚</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì €ëŠ¥ì•„ë‹ˆë¼á„€á„</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì™œ á„</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ë¬´ìŠ¬ë¦¼ì´ëƒ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ì´ì–ì•„á„á„á„á„á„á„á„á„á„á„á„á„á„</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì •ì‹ ë³‘ìì„</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì´ëŸ°ë³‘ì‹ ì´ë„¤</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì €ê±´ ë­ì§€</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì¼ë³¸ìœ¼ë¡œ ë³´ë‚´ì£¼ì„¸ìš”</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ë˜¥ê¼¬ì¶©ìƒˆë¼ì„</s>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ íŒ¨ì•…ì§ˆ..</s><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì§€ë“¤ì´ì—ˆì–ì•„</s>', '</s><s>ì¸ë“¤ì€ ë¹¡ì§€ë‹¤</s><pad>', '</s><s>ì¸ë“¤ì€ ì–´ê±°ì§„</s><pad><pad>', '</s><s>ì¸ë“¤ì€ ëŒ€ê¹¨ë¬¸ì„</s><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ á„á„á„á„á„á„á„á„á„á„á„</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì£½ì—¬ë²„ë¦¬ê³ ì‹¶ë‹¤</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ á„á„á„á„á„á„á„á„á„á„á„á„á„á„á„</s>', '</s><s>ì¸ë“¤ì€ ì¡°ì„ ì¡±ë“¤ì´êµ¬ë‚˜</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì´ê¸°ëŠ”ê²ƒë„ì—†ìŒ</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'] \n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-48affd9bb17a>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[0]) for key, val in item.items()}\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ë¬´ìŠ¨ ì¸ì¢…ì°¨ë³„</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ë¬´ìŠ¨</s><pad><pad><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ í•œêµ­ì¸ì´ëƒ?</s><pad><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì¢†ì¡±ìƒˆë¼ë“¤ì„</s>', '</s><s>ì¸ë“¤ì€ á„ƒá„ƒ</s><pad><pad><pad><pad>'] \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=9500, training_loss=0.15670610126696136, metrics={'train_runtime': 2043.5143, 'train_samples_per_second': 117.445, 'train_steps_per_second': 14.681, 'total_flos': 8234580185395200.0, 'train_loss': 0.15670610126696136, 'epoch': 5.063965884861407})"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# Print out what the model looks like before we start training\n","print(\"After training:\")\n","inputs_masked = [\"í•œêµ­ì¸ë“¤ì€ <mask>\"]\n","print(\"\\n Generations: \", tokenizer.batch_decode(model.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to('cuda').input_ids, do_sample=True, num_return_sequences = 3)),\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8NBYvrDlD9E","executionInfo":{"status":"ok","timestamp":1726754365723,"user_tz":-540,"elapsed":624,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"fafe1880-2a7b-49c5-a184-7fd247ad9ac0"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["After training:\n","\n"," Generations:  ['</s><s>ì¸ë“¤ì€ ì´ìŠ¬ëŒêµêµ¬ë‚˜</s><pad><pad><pad>', '</s><s>ì¸ë“¤ì€ ì¢†ê°™ì´ìƒê²»ë…¸</s>', '</s><s>ì¸ë“¤ì€ ë­í•˜ëƒ</s><pad><pad><pad>'] \n","\n"]}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3W1NCfA5lo4-","executionInfo":{"status":"ok","timestamp":1726754558269,"user_tz":-540,"elapsed":62122,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"eb5fc7d7-9ecc-44d2-e8c2-470a0f0f5c45"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) y\n","Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["MODEL_SAVE_HUB_PATH = 'koBART-toxic' # ex) 'my-bert-fine-tuned'\n","HUGGINGFACE_AUTH_TOKEN = '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' # https://huggingface.co/settings/token\n","\n","## Push to huggingface-hub\n","model.push_to_hub(\n","\t\t\tMODEL_SAVE_HUB_PATH,\n","\t\t\tuse_temp_dir=True,\n","\t\t\tuse_auth_token=HUGGINGFACE_AUTH_TOKEN\n",")\n","tokenizer.push_to_hub(\n","\t\t\tMODEL_SAVE_HUB_PATH,\n","\t\t\tuse_temp_dir=True,\n","\t\t\tuse_auth_token=HUGGINGFACE_AUTH_TOKEN\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262,"referenced_widgets":["ab397368248846748b6ead83b2c6c120","84b294acad8e48b48012260d453effe2","1c8774a19d844b28ba028a91c38a7111","127c055483984bc593c4c12e7c7e7fa2","ef71c5a5c0ac4232a0259d6e4f3bbbaf","5f473573a000407fb1a98bf1ac452f6a","1b4278870ca24d2fa59dbfa68d4b767b","1d6022b7492c4cf7866580add4093234","e964cdc642924fcea1b703265cf76786","6687efa93a914cf79ac9d3ec245025cd","2a5fcfa75acf4bc3a793381801c22a94","9c478eb1a330430b914c885d110ac34e","7f5aa818c7fb4a8eae8619d1996d08dc","4a49568624a44e0a804a317130ce923b","16056938be65493b84d1dfd29ae161e9","ad7ca83b6b784befa6f2fbdd34e7a89a","6bec89f4f2f246a7968af257f95ad3cc","411e0dea5a214d6a9ecbc958efcdef71","8ea886dee5784781b29ed24020e35bbd","35ddeb5173394684a2da229bfbeb2bfc","0c60578887b04018bd59d6b283d9cd36","ca5ebd8b6e744a85b0ca67937799152c"]},"id":"rMg4YKNsl5-U","executionInfo":{"status":"ok","timestamp":1726754766864,"user_tz":-540,"elapsed":67446,"user":{"displayName":"ë°•ì§€ìœ¤","userId":"00873494538360504322"}},"outputId":"4ab9a87c-7b19-4e08-e8fc-8b7cd097091b"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'forced_bos_token_id': 0, 'forced_eos_token_id': 1}\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab397368248846748b6ead83b2c6c120"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c478eb1a330430b914c885d110ac34e"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/cho-to/koBART-toxic/commit/e129e7cc8ef16b472d5294a14a1e9b711cf8cac3', commit_message='Upload tokenizer', commit_description='', oid='e129e7cc8ef16b472d5294a14a1e9b711cf8cac3', pr_url=None, pr_revision=None, pr_num=None)"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]}]}